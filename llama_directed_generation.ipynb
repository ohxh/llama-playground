{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "508845654f7e4126a45dc19824a94be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=31999)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from transformers import LLaMATokenizer, LlamaForCausalLM, GenerationConfig\n",
    "\n",
    "tokenizer = LLaMATokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
    "\n",
    "LOAD_8BIT = False\n",
    "BASE_MODEL = \"decapoda-research/llama-7b-hf\"\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_8bit=LOAD_8BIT,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"Model Loaded\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset amazon_polarity (/home/ubuntu/.cache/huggingface/datasets/amazon_polarity/amazon_polarity/3.0.0/a27b32b7e7b88eb274a8fa8ba0f654f1fe998a87c22547557317793b5d2772dc)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d589bf67e33641889503e55d1f7635a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from fsspec.utils import isfilelike, stringify_path\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# Let's just try IMDB for simplicity\n",
    "data = load_dataset(\"amazon_polarity\")[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_llama_hidden_states_from_ids(model,  ids, layer=-1):\n",
    "    input_ids = ids\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, output_hidden_states=True)\n",
    "\n",
    "    hs_tuple = output[\"hidden_states\"]\n",
    "    hs = hs_tuple[layer][0, -1].detach().cpu().numpy()\n",
    "\n",
    "    return hs\n",
    "\n",
    "def get_llama_hidden_states(model, tokenizer, text, layer=-1):\n",
    "    input_ids = tokenizer(text + tokenizer.eos_token, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    return get_llama_hidden_states_from_ids(model, input_ids, layer=-1)\n",
    "    \n",
    "\n",
    "def format_amazon(text, label):\n",
    "    return \"A customer wrote the following review:\\n{}\\nThe sentiment in this review is {}.\".format(text,  [\"negative\", \"positive\"][label])\n",
    "\n",
    "def format_amazon_for_completion(text):\n",
    "    return \"A customer wrote the following review:\\n{}\\nWhat is the sentiment of this review? Output only 'Postive' or 'Negative', and nothing else. the sentiment in this review is: \".format(text)\n",
    "\n",
    "def get_hidden_states_many_examples(model, tokenizer, data, n=200):\n",
    "\n",
    "    model.eval()\n",
    "    all_neg_hs, all_pos_hs, all_gt_labels, all_text = [], [], [], []\n",
    "\n",
    "    # loop\n",
    "    for _ in tqdm(range(n)):\n",
    "        # for simplicity, sample a random example until we find one that's a reasonable length\n",
    "        # (most examples should be a reasonable length, so this is just to make sure)\n",
    "        while True:\n",
    "            idx = np.random.randint(len(data))\n",
    "            text, true_label = data[idx][\"content\"], data[idx][\"label\"]\n",
    "            # the actual formatted input will be longer, so include a bit of a marign\n",
    "            if len(tokenizer(text)) < 400:  \n",
    "                break\n",
    "                \n",
    "        # get hidden states\n",
    "        neg_hs = get_llama_hidden_states(model, tokenizer, format_amazon(text, 0))\n",
    "        pos_hs = get_llama_hidden_states(model, tokenizer, format_amazon(text, 1))\n",
    "\n",
    "        # collect\n",
    "        all_neg_hs.append(neg_hs)\n",
    "        all_pos_hs.append(pos_hs)\n",
    "        all_gt_labels.append(true_label)\n",
    "        all_text.append(text)\n",
    "\n",
    "    all_neg_hs = np.stack(all_neg_hs)\n",
    "    all_pos_hs = np.stack(all_pos_hs)\n",
    "    all_gt_labels = np.stack(all_gt_labels)\n",
    "\n",
    "    return all_neg_hs, all_pos_hs, all_gt_labels, all_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:59<00:00,  8.35it/s]\n"
     ]
    }
   ],
   "source": [
    "neg_hs, pos_hs, y, all_text = get_hidden_states_many_examples(model, tokenizer, data, n=1000)\n",
    "\n",
    "n = len(y)\n",
    "neg_hs_train, neg_hs_test = neg_hs[:n//2], neg_hs[n//2:]\n",
    "pos_hs_train, pos_hs_test = pos_hs[:n//2], pos_hs[n//2:]\n",
    "text_train, text_test = all_text[:n//2], all_text[n//2:]\n",
    "y_train, y_test = y[:n//2], y[n//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the mean (and maybe also variance) of a data set\n",
    "def normalize(x, var_normalize = False):\n",
    "  normalized_x = x - x.mean(axis=0, keepdims=True)\n",
    "  if var_normalize:\n",
    "      normalized_x /= normalized_x.std(axis=0, keepdims=True)\n",
    "\n",
    "  return normalized_x\n",
    "\n",
    "# Collin's main loss function\n",
    "def ccs_loss(p0, p1):\n",
    "  return informative_loss(p0,p1) + consistent_loss(p0,p1)\n",
    "\n",
    "\n",
    "def informative_loss(p0, p1):\n",
    "  return (torch.min(p0, p1)**2).mean(0)\n",
    "\n",
    "\n",
    "def consistent_loss(p0, p1):\n",
    "  return ((p0 - (1-p1))**2).mean(0)\n",
    "\n",
    "\n",
    "def get_tensor_data(x0, x1):\n",
    "  x0 = torch.tensor(x0, dtype=torch.float, requires_grad=False, device=model.device)\n",
    "  x1 = torch.tensor(x1, dtype=torch.float, requires_grad=False, device=model.device)\n",
    "  return x0, x1\n",
    "\n",
    "\n",
    "def ccs(x0, x1, nepochs=200, ntries=10, lr=1e-3, batch_size=-1, \n",
    "              verbose=False, linear=True, weight_decay=0.01, var_normalize=False, loss_func=ccs_loss):\n",
    "\n",
    "    x0 = normalize(x0)\n",
    "    x1 = normalize(x1)\n",
    "\n",
    "    # Number of entries in the hidden states\n",
    "    d = x0.shape[-1]\n",
    "    \n",
    "    # probe\n",
    "    probe = nn.Sequential(nn.Linear(d, 1),nn.Sigmoid())\n",
    "    probe.to(model.device)  \n",
    "    best_probe = copy.deepcopy(probe)\n",
    "      \n",
    "    best_loss = np.inf\n",
    "\n",
    "    for train_num in range(ntries):\n",
    "        # Make a new probe for this run\n",
    "        probe = nn.Sequential(nn.Linear(d, 1), nn.Sigmoid())\n",
    "        probe.to(model.device)  \n",
    "\n",
    "        # ORder the data randomly in a tensor\n",
    "        x0, x1 = get_tensor_data(x0, x1)\n",
    "        permutation = torch.randperm(len(x0))\n",
    "        x0, x1 = x0[permutation], x1[permutation]\n",
    "        \n",
    "        # Set up optimizer. Collin uses adamW so that's what we'll go with\n",
    "        optimizer = torch.optim.AdamW(probe.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        # Start training\n",
    "        for epoch in range(nepochs):\n",
    "          # probe\n",
    "          p0, p1 = probe(x0), probe(x1)\n",
    "\n",
    "          # get the corresponding loss\n",
    "          loss = loss_func(p0, p1)\n",
    "\n",
    "          # update the parameters\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "        loss = loss.detach().cpu().item()\n",
    "        \n",
    "        if loss < best_loss:\n",
    "            best_probe = copy.deepcopy(probe)\n",
    "            best_loss = loss\n",
    "\n",
    "    return best_probe, best_loss\n",
    "\n",
    "\n",
    "def predict(probe, x0, x1):\n",
    "  x0 = torch.tensor(normalize(x0), dtype=torch.float, requires_grad=False, device=model.device)\n",
    "  x1 = torch.tensor(normalize(x1), dtype=torch.float, requires_grad=False, device=model.device)\n",
    "  \n",
    "  with torch.no_grad():\n",
    "      p0, p1 = probe(x0), probe(x1)\n",
    "\n",
    "  avg_confidence = p0 - p1\n",
    "  predictions = (avg_confidence.detach().cpu().numpy())[:, 0]\n",
    "\n",
    "  \n",
    "  return predictions\n",
    "\n",
    "def predict_pair(probe, x0, x1):\n",
    "  x0 = torch.tensor(x0, dtype=torch.float, requires_grad=False, device=model.device)\n",
    "  x1 = torch.tensor(x1, dtype=torch.float, requires_grad=False, device=model.device)\n",
    "  \n",
    "  with torch.no_grad():\n",
    "      p0, p1 = probe(x0), probe(x1)\n",
    "\n",
    "  avg_confidence = p0 - p1\n",
    "  predictions = (avg_confidence.detach().cpu().numpy())[:,0]\n",
    "  return predictions\n",
    "\n",
    "def classify_single(classifier_direction, hs):\n",
    "  confidences = np.apply_along_axis(lambda x : np.dot(x,classifier_direction), 1, hs)\n",
    "\n",
    "  return confidences\n",
    "\n",
    "def predict_single(probe, x0):\n",
    "  x0 = torch.tensor(normalize(x0), dtype=torch.float, requires_grad=False, device=model.device)\n",
    "  \n",
    "  with torch.no_grad():\n",
    "      p0 = probe(x0)\n",
    "\n",
    "  avg_confidence = p0\n",
    "  predictions = (avg_confidence.detach().cpu().numpy())[:,0]\n",
    "  return predictions\n",
    "\n",
    "def get_acc(probe, x0_test, x1_test, y_test):\n",
    "  predictions = (predict(probe, x0_test, x1_test) < 0.5).astype(int)\n",
    "\n",
    "  # If predictions get messed up (i.e. ever not 1 or 0) this method will show\n",
    "  # really good accuracy. TODO evaluate vs y_test and y_test inverted to avoid\n",
    "  # this.\n",
    "  acc = (predictions == y_test).mean()\n",
    "  return acc\n",
    "  acc = max(acc, 1 - acc)\n",
    "\n",
    "  return acc\n",
    "\n",
    "def is_reversed(probe, x0_test, x1_test, y_test):\n",
    "  predictions = (predict(probe, x0_test, x1_test) < 0.5).astype(int)\n",
    "\n",
    "  # If predictions get messed up (i.e. ever not 1 or 0) this method will show\n",
    "  # really good accuracy. TODO evaluate vs y_test and y_test inverted to avoid\n",
    "  # this.\n",
    "  acc = (predictions == y_test).mean()\n",
    "\n",
    "  return acc < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "x_train = neg_hs_train - pos_hs_train\n",
    "x_test = neg_hs_test - pos_hs_test\n",
    "\n",
    "\n",
    "lr = LogisticRegression(class_weight=\"balanced\", max_iter=1000)\n",
    "lr.fit(x_train, y_train)\n",
    "print(\"Logistic regression accuracy: {}\".format(lr.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCS Accuracy: 0.04, loss: 0.012944226153194904\n",
      "Flipping direction\n"
     ]
    }
   ],
   "source": [
    "probe, loss = ccs(neg_hs_train, pos_hs_train, ntries=1)\n",
    "\n",
    "ccs_acc = get_acc(probe, neg_hs_test, pos_hs_test, y_test)\n",
    "\n",
    "print(\"CCS Accuracy: {}, loss: {}\".format(ccs_acc, loss))\n",
    "\n",
    "classifier_direction = np.squeeze(np.transpose(probe[0].weight.detach().cpu().numpy()))\n",
    "\n",
    "if(is_reversed(probe, neg_hs_test, pos_hs_test, y_test)):\n",
    "  print(\"Flipping direction\")\n",
    "  classifier_direction = -classifier_direction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1, 11131,   350,  3615,   338],\n",
      "        [    1, 11131,   350,  3615,   338],\n",
      "        [    1, 11131,   350,  3615,   338],\n",
      "        [    1, 11131,   350,  3615,   338]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_990467/2910365738.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  classified = F.softmax(torch.tensor(classified, device=model.device, dtype=torch.half))\n",
      "/tmp/ipykernel_990467/2910365738.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(scores[i][top_k[i]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Joe Biden is a : 0.080810546875 x 0.30712890625 = 0.387939453125\n",
      " Joe Biden is the : 0.270751953125 x 0.266845703125 = 0.53759765625\n",
      " Joe Biden is not : 0.2034912109375 x 0.0966796875 = 0.30029296875\n",
      " Joe Biden is running : 0.00841522216796875 x 0.0880126953125 = 0.096435546875\n",
      " Joe Biden is in : 0.0701904296875 x 0.062408447265625 = 0.132568359375\n",
      " Joe Biden is an : 0.278076171875 x 0.057708740234375 = 0.335693359375\n",
      " Joe Biden is one : 0.04852294921875 x 0.034454345703125 = 0.0830078125\n",
      " Joe Biden is going : 0.00556182861328125 x 0.0297088623046875 = 0.0352783203125\n",
      " Joe Biden is now : 0.0174102783203125 x 0.0287933349609375 = 0.04620361328125\n",
      " Joe Biden is still : 0.0167999267578125 x 0.02813720703125 = 0.044921875\n",
      "tensor([[    1, 11131,   350,  3615,   338,   263],\n",
      "        [    1, 11131,   350,  3615,   338,   278],\n",
      "        [    1, 11131,   350,  3615,   338,   451],\n",
      "        [    1, 11131,   350,  3615,   338,  2734]], device='cuda:0')\n",
      " Joe Biden is running for : 0.0214385986328125 x 0.857421875 = 0.87890625\n",
      " Joe Biden is running. : 0.034515380859375 x 0.032196044921875 = 0.06671142578125\n",
      " Joe Biden is running to : 0.0374755859375 x 0.0279693603515625 = 0.0654296875\n",
      " Joe Biden is running in : 0.1280517578125 x 0.0271148681640625 = 0.1551513671875\n",
      " Joe Biden is running a : 0.0693359375 x 0.0152435302734375 = 0.0845947265625\n",
      " Joe Biden is running, : 0.01444244384765625 x 0.0147705078125 = 0.029205322265625\n",
      " Joe Biden is running as : 0.2481689453125 x 0.0082855224609375 = 0.25634765625\n",
      " Joe Biden is running again : 0.00726318359375 x 0.005970001220703125 = 0.0132293701171875\n",
      " Joe Biden is running\n",
      " : 0.11700439453125 x 0.0057830810546875 = 0.122802734375\n",
      " Joe Biden is running against : 0.322509765625 x 0.005390167236328125 = 0.327880859375\n",
      "tensor([[    1, 11131,   350,  3615,   338,  2734,   363],\n",
      "        [    1, 11131,   350,  3615,   338,   263,   767],\n",
      "        [    1, 11131,   350,  3615,   338,   263,   619],\n",
      "        [    1, 11131,   350,  3615,   338,   278,  1556]], device='cuda:0')\n",
      " Joe Biden is the most likely : 0.0040130615234375 x 0.1905517578125 = 0.194580078125\n",
      " Joe Biden is the most qualified : 0.0635986328125 x 0.173583984375 = 0.2371826171875\n",
      " Joe Biden is the most popular : 0.034454345703125 x 0.1605224609375 = 0.1949462890625\n",
      " Joe Biden is the most dangerous : 0.06927490234375 x 0.1309814453125 = 0.2001953125\n",
      " Joe Biden is the most recent : 0.022857666015625 x 0.0706787109375 = 0.093505859375\n",
      " Joe Biden is the most liberal : 0.07733154296875 x 0.069580078125 = 0.14697265625\n",
      " Joe Biden is the most elect : 0.43701171875 x 0.06903076171875 = 0.505859375\n",
      " Joe Biden is the most experienced : 0.05596923828125 x 0.04632568359375 = 0.102294921875\n",
      " Joe Biden is the most cor : 0.2158203125 x 0.045989990234375 = 0.26171875\n",
      " Joe Biden is the most un : 0.01959228515625 x 0.0428466796875 = 0.06243896484375\n",
      "tensor([[    1, 11131,   350,  3615,   338,  2734,   363,  6673],\n",
      "        [    1, 11131,   350,  3615,   338,   263,   619,   279],\n",
      "        [    1, 11131,   350,  3615,   338,  2734,   363,  7178],\n",
      "        [    1, 11131,   350,  3615,   338,  2734,   363,   278]],\n",
      "       device='cuda:0')\n",
      " Joe Biden is running for the Democratic : 0.016632080078125 x 0.431640625 = 0.4482421875\n",
      " Joe Biden is running for the White : 0.27685546875 x 0.203857421875 = 0.480712890625\n",
      " Joe Biden is running for the  : 0.0237274169921875 x 0.1314697265625 = 0.1551513671875\n",
      " Joe Biden is running for the presiden : 0.279296875 x 0.100830078125 = 0.380126953125\n",
      " Joe Biden is running for the Dem : 0.01378631591796875 x 0.032470703125 = 0.0462646484375\n",
      " Joe Biden is running for the Pres : 0.09039306640625 x 0.028656005859375 = 0.1190185546875\n",
      " Joe Biden is running for the president : 0.1123046875 x 0.0284423828125 = 0.1407470703125\n",
      " Joe Biden is running for the Democr : 0.043212890625 x 0.0176849365234375 = 0.0609130859375\n",
      " Joe Biden is running for the dem : 0.094970703125 x 0.013885498046875 = 0.10888671875\n",
      " Joe Biden is running for the nom : 0.04876708984375 x 0.0111541748046875 = 0.0599365234375\n",
      "tensor([[    1, 11131,   350,  3615,   338,  2734,   363,  6673, 29889],\n",
      "        [    1, 11131,   350,  3615,   338,  2734,   363,  6673,   297],\n",
      "        [    1, 11131,   350,  3615,   338,  2734,   363,  6673, 29892],\n",
      "        [    1, 11131,   350,  3615,   338,  2734,   363,  6673,  1363]],\n",
      "       device='cuda:0')\n",
      " Joe Biden is running for president because he : 0.001728057861328125 x 0.69189453125 = 0.69384765625\n",
      " Joe Biden is running for president because of : 0.35546875 x 0.081298828125 = 0.436767578125\n",
      " Joe Biden is running for president because his : 0.1712646484375 x 0.058990478515625 = 0.230224609375\n",
      " Joe Biden is running for president because the : 0.06292724609375 x 0.04852294921875 = 0.1114501953125\n",
      " Joe Biden is running for president because, : 3.1054019927978516e-05 x 0.03692626953125 = 0.036956787109375\n",
      " Joe Biden is running for president because it : 0.2484130859375 x 0.0289764404296875 = 0.27734375\n",
      " Joe Biden is running for president because Donald : 0.11083984375 x 0.021240234375 = 0.132080078125\n",
      " Joe Biden is running for president because “ : 0.048309326171875 x 0.01119232177734375 = 0.05950927734375\n",
      " Joe Biden is running for president because we : 0.0003523826599121094 x 0.01093292236328125 = 0.01128387451171875\n",
      " Joe Biden is running for president because America : 0.0007758140563964844 x 0.01003265380859375 = 0.01081085205078125\n",
      "tensor([[    1, 11131,   350,  3615,   338,  2734,   363,  6673,   297, 29871],\n",
      "        [    1, 11131,   350,  3615,   338,  2734,   363,  6673,  1363,   540],\n",
      "        [    1, 11131,   350,  3615,   338,  2734,   363,  6673, 29889,    13],\n",
      "        [    1, 11131,   350,  3615,   338,  2734,   363,  6673, 29892,   322]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from transformers import LogitsProcessorList, LogitsProcessor\n",
    "\n",
    "def sigmoid(x):\n",
    "      return 1/(1 + np.exp(-x))\n",
    "    \n",
    "class HiddenStateDirectedLogitsProcessor(LogitsProcessor):\n",
    "  def __init__(self, **kwargs):\n",
    "     pass\n",
    "\n",
    "  def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "    k = 10\n",
    "    top_k = torch.argsort(scores, descending=True)[:,0:k]\n",
    "\n",
    "    print(input_ids)\n",
    "\n",
    "    for i, sequence in enumerate(input_ids):\n",
    "\n",
    "      # Stick all possible next tokens on the end\n",
    "      completions = torch.cat([sequence.repeat(k,1), top_k[i].reshape(-1,1)],1)\n",
    "\n",
    "      # Get hidden states\n",
    "      hss = [get_llama_hidden_states_from_ids(model,x.unsqueeze(0),-1) for x in completions]\n",
    "\n",
    "      # print(\"classified\")\n",
    "      classified = classify_single(classifier_direction, hss)\n",
    "\n",
    "      # classified = classified - classified.mean()\n",
    "      classified = F.softmax(torch.tensor(classified, device=model.device, dtype=torch.half))\n",
    "      # print(classified)\n",
    "      # print(\"truth\")\n",
    "      # truthiness = [sigmoid(x/20) for x in classified]\n",
    "      # print(truthiness)\n",
    "      # print(\"score\")\n",
    "      # print(scores[i][top_k[i]])\n",
    "      # print(\"softmax\")\n",
    "      # print(F.softmax(scores[i])[top_k[i]])\n",
    "\n",
    "      probs = F.softmax(scores[i][top_k[i]])\n",
    "    \n",
    "      scores[i] = torch.zeros(scores[i].size(), device=model.device, dtype=torch.half)\n",
    "\n",
    "      # print(\"probsraw\")\n",
    "      \n",
    "      # print(probs)\n",
    "\n",
    "      # probs = probs / probs.sum()\n",
    "\n",
    "      # print(\"probs\")\n",
    "      # print(probs)\n",
    "\n",
    "      scores[i][top_k[i]] =  probs\n",
    "\n",
    "      # print(\"Top scores\")\n",
    "      # print(scores[i][top_k[i]])\n",
    "      # print(\"All scores\")\n",
    "      # print(scores)\n",
    "      # print(scores[i])\n",
    "      # scores[i] = torch.zeros(scores[i.size])\n",
    "      # print(\"zerod\")\n",
    "      # print(scores[i])\n",
    "      # scores[i][top_k[i]] = [truthiness[j] * probs[j] for j in range(len(truthiness))]\n",
    "      # print(\"merged\")\n",
    "      # print(scores[i])\n",
    "\n",
    "    # for j in range(len(completions)):\n",
    "    #     print(\"{} : {} x {} = {}\".format(tokenizer.decode(completions[j]), classified[j], probs[j], probs[j] + classified[j]))\n",
    "    return scores\n",
    "     \n",
    "\n",
    "def generate(\n",
    "    text,\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    num_beams=4,\n",
    "    max_new_tokens=12,\n",
    "    **kwargs,\n",
    "):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        num_beams=num_beams,\n",
    "        **kwargs,\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            logits_processor=LogitsProcessorList([HiddenStateDirectedLogitsProcessor()])\n",
    "        )\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "    print(generation_output)\n",
    "    return output\n",
    "\n",
    "print(generate(\"Joe Biden is\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
